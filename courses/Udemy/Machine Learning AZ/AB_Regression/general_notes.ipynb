{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simple Linear Regression\n",
    "\n",
    "`y = b0 + b1 * x1`\n",
    "\n",
    "- y  => dependent variable\n",
    "- x1 => independent variable\n",
    "- b0 => constant\n",
    "- b1 => coefficient"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The best fit line is which has the minimum sum of **\"ordinary least squares\"**\n",
    "\n",
    "`MIN(SUM(Y - Y_PREDICTED)^2)`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multi-Linear Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`y = b0 + b1*x1 + b2*x2 + ... + bn*xn`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Assumptions of a linear regression (you need to check each one before applying it on the data)**\n",
    "\n",
    "1. Linearity\n",
    "2. Homoscedasticity\n",
    "3. Multivariate normality\n",
    "4. Independence of errors\n",
    "5. Lack of multicollinearity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Dummy variable trap**\n",
    "\n",
    "Let's say that you have a categorical variable (i.e. American State names) which has two categories in the dataset.\n",
    "\n",
    "You can create a new column to encode this categorical variable as a dummy variable, but you don't need to include two new dummy vars. because the second would be duplicating information.\n",
    "\n",
    "\n",
    "`y = b0 + b1*x1 + b2*x2 + b3*x3 (after here it comes dummy vars) + b4*D1`\n",
    "\n",
    "We don't include `b5*D2`, because `D2 = 1 - D1`. When a independent variable predicts another independent, that's called multicollinearity. \n",
    "\n",
    "So, you need to exclude a dummy variable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Statistical Significance (intuition)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You assume that you live in an `H_0` universe (called the null hypothesis) and there is an alternative one `H_1`.\n",
    "\n",
    "Then you run the experiment, and you will rejecting the null hypothesis if the probably of the null hypothesis being true is less than 5% (this threshold is called the P-value).\n",
    "\n",
    "In a nutshell, statistical significance it's the point where the human intuitive terms you get uneasy about the null hypothesis being true."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Building a model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Five methods to discard (or not 😏) independent variables in order to build a better model**\n",
    "\n",
    "1. All-in (by prior domanin knowledge or you have to because any reason)\n",
    "2. Backward Elimination\n",
    "    - 2.1 Select a significance level to stay in the model (e.g. SL = 0.05).\n",
    "    - 2.2 Fit the fill model with all possible predictors.\n",
    "    - 2.3 Consider the predictor with the highest P-value. If `P > SL`, go to STEP 4, otherwise go to FIN.\n",
    "    - 2.4 Remove the predictor.\n",
    "    - 2.5 Fit model without this variable. Go to 2.3 step.\n",
    "    - 2.6 FIN.\n",
    "3. Forward Selection\n",
    "    - 3.1 Select significance level to enter the model (e.g. SL = 0.05).\n",
    "    - 3.2 Fit all simple regression models `y ~ x_n`. Select the one with the lowest P-value.\n",
    "    - 3.3 Keep this variable and fit all possible models with one extra predictor added to the one(s) you already have.\n",
    "    - 3.4 Consider thr predictor with the lowest P-value. If `P < SL`, go to STEP 3.3, otherwise go to FIN.\n",
    "    - 3.5 FIN. Keep the previous model.\n",
    "4. Bidirectional Elimination\n",
    "    - 4.1 Select a significance level to enter and to stay in the model e.g.: SLENTER = 0.05, SLSTAY = 0.05.\n",
    "    - 4.2 Perform the next step of Forward selection (new variables must have: `P < SLENTER` to enter).\n",
    "    - 4.3 Perform ALL steps of Backward Elimination (old variables must have: `P < SLSTAY` to stay). Go to Step 4.2 or 4.4.\n",
    "    - 4.4 No new variables can enter and no old variables can exit.\n",
    "    - 4.5 FIN.\n",
    "5. Score Comparison - all possible methods.\n",
    "    - 5.1 Select a criterion of goodness of fit.\n",
    "    - 5.2 Construct all possible regression models: `2^n - 1` total combinations.\n",
    "    - 5.3 Select the one with the best criterion.\n",
    "    - 5.4 FIN.\n",
    "\n",
    "\n",
    "1), 2) and 3) are called stepwise regression."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Important\n",
    "\n",
    "In multiple linear-regression is not necessary to apply feature scaling, because the coefficient will compensate higher values of features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Polynomial Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`y = b0 + b1*x1 + b2*x1² + ... + bn*xn^n`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Polinomial regression is just a special version of multiple linear regression.\n",
    "\n",
    "It's called linear for Polynomial \"Linear\" Regression because we are talking about the coefficients in the function, and if the function can be expressed as a linear combination of coefficients."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Support Vector Regression (SVR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instead of modeling data with a simple linear curve. We now use a tube, a tube with epsilon width, which allows certain error margin, and the points outside this tube are drawn as support vectors. Because they are guiding the form and location of the tube."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![SVR intuition](./assets/SVR%20intuition.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regression Decision Tree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The algorithm split the datasets until the leafs don't add more information (related to information entropy), then maps the leafs to the average of the corresponding targets (Y)."
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "d22c8c23b537271198e05d0d66c67cdcf87861383a23715c93639a0fbcbd75aa"
  },
  "kernelspec": {
   "display_name": "Python 3.9.12 ('ml_az')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
