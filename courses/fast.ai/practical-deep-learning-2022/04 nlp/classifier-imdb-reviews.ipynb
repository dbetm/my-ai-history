{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"colab":{"provenance":[],"gpuType":"T4"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"We will create a classifier (text model) of IMDB movie reviews.","metadata":{"id":"KiooukDsYbLu"}},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nfrom fastai.text.all import (\n    coll_repr,\n    defaults,\n    first,\n    get_text_files,\n    Numericalize,\n    L,\n    LMDataLoader, # Language Model Data Loader\n    Tokenizer,\n    untar_data,\n    URLs,\n    WordTokenizer,\n)","metadata":{"id":"AGXHLfGGYbL1","execution":{"iopub.status.busy":"2023-06-04T18:15:03.406780Z","iopub.execute_input":"2023-06-04T18:15:03.407124Z","iopub.status.idle":"2023-06-04T18:15:12.316397Z","shell.execute_reply.started":"2023-06-04T18:15:03.407096Z","shell.execute_reply":"2023-06-04T18:15:12.315405Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"path = untar_data(URLs.IMDB)\nfiles = get_text_files(path, folders = ['train', 'test', 'unsup'])\n\nprint(\"Data downloaded at\", path)","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":55},"id":"N1Tm0Ym6YbL7","outputId":"24d04f04-4c0e-4900-ab83-20790d467146","execution":{"iopub.status.busy":"2023-06-04T18:15:12.318606Z","iopub.execute_input":"2023-06-04T18:15:12.319248Z","iopub.status.idle":"2023-06-04T18:15:42.979547Z","shell.execute_reply.started":"2023-06-04T18:15:12.319214Z","shell.execute_reply":"2023-06-04T18:15:42.978572Z"},"trusted":true},"execution_count":2,"outputs":[{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n<style>\n    /* Turns off some styling */\n    progress {\n        /* gets rid of default border in Firefox and Opera. */\n        border: none;\n        /* Needs to be in here for Safari polyfill so background images work as expected. */\n        background-size: auto;\n    }\n    progress:not([value]), progress:not([value])::-webkit-progress-bar {\n        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);\n    }\n    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n        background: #F44336;\n    }\n</style>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      <progress value='144441344' class='' max='144440600' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      100.00% [144441344/144440600 00:02&lt;00:00]\n    </div>\n    "},"metadata":{}},{"name":"stdout","text":"Data downloaded at /root/.fastai/data/imdb\n","output_type":"stream"}]},{"cell_type":"code","source":"word_tokenizer = Tokenizer(tok=WordTokenizer())","metadata":{"id":"BzhXYRGWYbL-","execution":{"iopub.status.busy":"2023-06-04T18:27:24.366376Z","iopub.execute_input":"2023-06-04T18:27:24.366793Z","iopub.status.idle":"2023-06-04T18:27:24.636885Z","shell.execute_reply.started":"2023-06-04T18:27:24.366761Z","shell.execute_reply":"2023-06-04T18:27:24.635940Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"markdown","source":"### Creating text batches","metadata":{"id":"6GUBq4ZJYbL_"}},{"cell_type":"code","source":"# Imagine we have a text stream, the tokenization process will add special tokens and deal with\n# punctuation.\n\n# We now have 90 tokens, separated by spaces. Let's say we want a batch size of 6. We need to \n# break this text into 6 contiguous parts of length 15:\n\nstream = (\n    \"In this chapter, we will go back over the example of classifying movie reviews we studied in chapter 1 and dig deeper under the surface. First we will look at the processing steps necessary to convert text into numbers and how to customize it. By doing this, we'll have another example of the PreProcessor used in the data block API.\\nThen we will study how we build a language model and train it for a while.\"\n)\ntokens = word_tokenizer(stream)\nbatch_size, seq_len = 6, 15\n\nd_tokens = np.array([tokens[i*seq_len:(i+1)*seq_len] for i in range(batch_size)])\ndf = pd.DataFrame(d_tokens)\ndf\n","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":237},"id":"pi1tQm94YbMA","outputId":"6d0dc36f-e470-4053-c689-43510c32dbf9","execution":{"iopub.status.busy":"2023-06-04T18:27:44.161112Z","iopub.execute_input":"2023-06-04T18:27:44.161521Z","iopub.status.idle":"2023-06-04T18:27:44.189277Z","shell.execute_reply.started":"2023-06-04T18:27:44.161492Z","shell.execute_reply":"2023-06-04T18:27:44.188326Z"},"trusted":true},"execution_count":13,"outputs":[{"execution_count":13,"output_type":"execute_result","data":{"text/plain":"      0        1             2        3        4        5           6      7   \\\n0  xxbos    xxmaj            in     this  chapter        ,          we   will   \n1  movie  reviews            we  studied       in  chapter           1    and   \n2  first       we          will     look       at      the  processing  steps   \n3    how       to     customize       it        .    xxmaj          by  doing   \n4     of      the  preprocessor     used       in      the        data  block   \n5   will    study           how       we    build        a    language  model   \n\n          8       9        10    11       12       13           14  \n0         go    back     over   the  example       of  classifying  \n1        dig  deeper    under   the  surface        .        xxmaj  \n2  necessary      to  convert  text     into  numbers          and  \n3       this       ,       we   'll     have  another      example  \n4       xxup     api        .    \\n    xxmaj     then           we  \n5        and   train       it   for        a    while            .  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>0</th>\n      <th>1</th>\n      <th>2</th>\n      <th>3</th>\n      <th>4</th>\n      <th>5</th>\n      <th>6</th>\n      <th>7</th>\n      <th>8</th>\n      <th>9</th>\n      <th>10</th>\n      <th>11</th>\n      <th>12</th>\n      <th>13</th>\n      <th>14</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>xxbos</td>\n      <td>xxmaj</td>\n      <td>in</td>\n      <td>this</td>\n      <td>chapter</td>\n      <td>,</td>\n      <td>we</td>\n      <td>will</td>\n      <td>go</td>\n      <td>back</td>\n      <td>over</td>\n      <td>the</td>\n      <td>example</td>\n      <td>of</td>\n      <td>classifying</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>movie</td>\n      <td>reviews</td>\n      <td>we</td>\n      <td>studied</td>\n      <td>in</td>\n      <td>chapter</td>\n      <td>1</td>\n      <td>and</td>\n      <td>dig</td>\n      <td>deeper</td>\n      <td>under</td>\n      <td>the</td>\n      <td>surface</td>\n      <td>.</td>\n      <td>xxmaj</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>first</td>\n      <td>we</td>\n      <td>will</td>\n      <td>look</td>\n      <td>at</td>\n      <td>the</td>\n      <td>processing</td>\n      <td>steps</td>\n      <td>necessary</td>\n      <td>to</td>\n      <td>convert</td>\n      <td>text</td>\n      <td>into</td>\n      <td>numbers</td>\n      <td>and</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>how</td>\n      <td>to</td>\n      <td>customize</td>\n      <td>it</td>\n      <td>.</td>\n      <td>xxmaj</td>\n      <td>by</td>\n      <td>doing</td>\n      <td>this</td>\n      <td>,</td>\n      <td>we</td>\n      <td>'ll</td>\n      <td>have</td>\n      <td>another</td>\n      <td>example</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>of</td>\n      <td>the</td>\n      <td>preprocessor</td>\n      <td>used</td>\n      <td>in</td>\n      <td>the</td>\n      <td>data</td>\n      <td>block</td>\n      <td>xxup</td>\n      <td>api</td>\n      <td>.</td>\n      <td>\\n</td>\n      <td>xxmaj</td>\n      <td>then</td>\n      <td>we</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>will</td>\n      <td>study</td>\n      <td>how</td>\n      <td>we</td>\n      <td>build</td>\n      <td>a</td>\n      <td>language</td>\n      <td>model</td>\n      <td>and</td>\n      <td>train</td>\n      <td>it</td>\n      <td>for</td>\n      <td>a</td>\n      <td>while</td>\n      <td>.</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"markdown","source":"It is important to maintain order within and across these subarrays, because we will use a model that maintains a state so that it remembers what it read previously when predicting what comes next.\n\nThe first step is to transform the individual texts into a stream by concatenating them together. As with images, it's best to randomize the order of the inputs, so at the beginning of each epoch we will shuffle the entries to make a new stream (we shuffle the order of the documents, not the order of the words inside them).","metadata":{"id":"HPYLaa93YbMB"}},{"cell_type":"code","source":"\"\"\"So to recap, at every epoch we shuffle our collection of documents and concatenate them into a stream of tokens. \nWe then cut that stream into a batch of fixed-size consecutive mini-streams. Our model will then read the \nmini-streams in order, and thanks to an inner state, it will produce the same activation whatever sequence \nlength we picked.\n\"\"\"\nnumericalizer = Numericalize()","metadata":{"id":"7C6Ve8_AYbMD","execution":{"iopub.status.busy":"2023-06-04T18:27:49.194957Z","iopub.execute_input":"2023-06-04T18:27:49.195326Z","iopub.status.idle":"2023-06-04T18:27:49.201186Z","shell.execute_reply.started":"2023-06-04T18:27:49.195297Z","shell.execute_reply":"2023-06-04T18:27:49.200118Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"# example\n\n# to demostrate this method we will select a corpus of 2000 movie reviews\ntxts = L(o.open().read() for o in files[:2000])\n\n# Just like SubwordTokenizer we need to call setup on Numeralize\ntoks200 = txts[:200].map(word_tokenizer)\ntoks200[0]","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"6m0P8WptYbMF","outputId":"ab8813c5-be65-48be-ecf0-ad523d9414ff","execution":{"iopub.status.busy":"2023-06-04T18:27:51.000292Z","iopub.execute_input":"2023-06-04T18:27:51.000632Z","iopub.status.idle":"2023-06-04T18:27:52.688468Z","shell.execute_reply.started":"2023-06-04T18:27:51.000605Z","shell.execute_reply":"2023-06-04T18:27:52.687576Z"},"trusted":true},"execution_count":15,"outputs":[{"execution_count":15,"output_type":"execute_result","data":{"text/plain":"(#158) ['xxbos','xxmaj','jiang','xxmaj','xian','uses','the','complex','backstory','of'...]"},"metadata":{}}]},{"cell_type":"code","source":"numericalizer.setup(toks200)","metadata":{"id":"DtIgf2IpYbMG","execution":{"iopub.status.busy":"2023-06-04T18:27:57.070597Z","iopub.execute_input":"2023-06-04T18:27:57.070962Z","iopub.status.idle":"2023-06-04T18:27:57.089999Z","shell.execute_reply.started":"2023-06-04T18:27:57.070935Z","shell.execute_reply":"2023-06-04T18:27:57.089084Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"code","source":"nums200 = toks200.map(numericalizer)\n\ndl = LMDataLoader(nums200)\n\nx,y = first(dl)\nx.shape,y.shape","metadata":{"id":"m9l7WapjYbMI","execution":{"iopub.status.busy":"2023-06-04T18:28:02.571784Z","iopub.execute_input":"2023-06-04T18:28:02.572841Z","iopub.status.idle":"2023-06-04T18:28:02.670160Z","shell.execute_reply.started":"2023-06-04T18:28:02.572798Z","shell.execute_reply":"2023-06-04T18:28:02.669087Z"},"trusted":true},"execution_count":17,"outputs":[{"execution_count":17,"output_type":"execute_result","data":{"text/plain":"(torch.Size([64, 72]), torch.Size([64, 72]))"},"metadata":{}}]},{"cell_type":"code","source":"\" \".join(numericalizer.vocab[o] for o in x[0][:20])","metadata":{"id":"AM7CqWIbYbMJ","execution":{"iopub.status.busy":"2023-06-04T18:28:07.402971Z","iopub.execute_input":"2023-06-04T18:28:07.403326Z","iopub.status.idle":"2023-06-04T18:28:07.411384Z","shell.execute_reply.started":"2023-06-04T18:28:07.403299Z","shell.execute_reply":"2023-06-04T18:28:07.410469Z"},"trusted":true},"execution_count":18,"outputs":[{"execution_count":18,"output_type":"execute_result","data":{"text/plain":"'xxbos xxmaj xxunk xxmaj xxunk uses the complex xxunk of xxmaj xxunk xxmaj xxunk and xxmaj xxunk xxmaj xxunk to'"},"metadata":{}}]},{"cell_type":"markdown","source":"### Training a Text Classifier\n\nThere are two steps to training a state-of-the-art text classifier using transfer learning: first we need to fine-tune our language model pretrained on Wikipedia to the corpus of IMDb reviews, and then we can use that model to train a classifier.","metadata":{"id":"cWkGTJpXYbMK"}},{"cell_type":"code","source":"from functools import partial\n\nfrom fastai.text.all import (\n    accuracy,\n    language_model_learner,\n    AWD_LSTM,\n    DataBlock,\n    TextBlock,\n    Perplexity,\n    RandomSplitter,\n)","metadata":{"id":"wesnh4CuYbML","execution":{"iopub.status.busy":"2023-06-04T18:16:06.299837Z","iopub.execute_input":"2023-06-04T18:16:06.300199Z","iopub.status.idle":"2023-06-04T18:16:06.505421Z","shell.execute_reply.started":"2023-06-04T18:16:06.300153Z","shell.execute_reply":"2023-06-04T18:16:06.504196Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"# fastai handles tokenization and numericalization automatically when TextBlock is passed to DataBlock\nget_imdb = partial(get_text_files, folders=[\"train\", \"test\", \"unsup\"])\nbatch_size = 128\nseq_len = 80\n\ndls_lm = DataBlock(\n    blocks=TextBlock.from_folder(path, is_lm=True),\n    get_items=get_imdb,\n    splitter=RandomSplitter(0.1)\n).dataloaders(path, path=path, bs=batch_size, seq_len=seq_len)","metadata":{"id":"7_BeG4yzYbML","execution":{"iopub.status.busy":"2023-06-04T18:28:13.564086Z","iopub.execute_input":"2023-06-04T18:28:13.564939Z","iopub.status.idle":"2023-06-04T18:28:19.433785Z","shell.execute_reply.started":"2023-06-04T18:28:13.564905Z","shell.execute_reply":"2023-06-04T18:28:19.432846Z"},"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"code","source":"# we then can show a pair of example\ndls_lm.show_batch(max_n=2)","metadata":{"id":"PI5BfPI6YbMM","execution":{"iopub.status.busy":"2023-06-04T18:28:25.938326Z","iopub.execute_input":"2023-06-04T18:28:25.938699Z","iopub.status.idle":"2023-06-04T18:28:26.755238Z","shell.execute_reply.started":"2023-06-04T18:28:25.938664Z","shell.execute_reply":"2023-06-04T18:28:26.754262Z"},"trusted":true},"execution_count":20,"outputs":[{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>text</th>\n      <th>text_</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>xxbos xxmaj huh ? \\n\\n xxmaj what ? \\n\\n xxmaj vampire cavemen ? xxmaj sex replaced by flashing multi - colored light bulbs ? xxmaj guys in dinosaur suits ? a film half made of stock footage ? \\n\\n xxmaj this is n't just bad , it 's inexplicably bad . xxup do xxup not xxup watch xxup this xxup alone . xxmaj make sure to have a friend or two with whom you can swap wisecracks about this …</td>\n      <td>xxmaj huh ? \\n\\n xxmaj what ? \\n\\n xxmaj vampire cavemen ? xxmaj sex replaced by flashing multi - colored light bulbs ? xxmaj guys in dinosaur suits ? a film half made of stock footage ? \\n\\n xxmaj this is n't just bad , it 's inexplicably bad . xxup do xxup not xxup watch xxup this xxup alone . xxmaj make sure to have a friend or two with whom you can swap wisecracks about this … this</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>3 0 , while he 's 40 . xxmaj besides , many transitions take place from 2 xxrep 3 0 to the 70 's or the other way around without any warning . xxmaj this is to show that the character did n't really evolved much . xxmaj he was a dreamer when younger , and unlike many he did n't change when he grew up . \\n\\n xxmaj about transitions , they all are very very smooth , and</td>\n      <td>0 , while he 's 40 . xxmaj besides , many transitions take place from 2 xxrep 3 0 to the 70 's or the other way around without any warning . xxmaj this is to show that the character did n't really evolved much . xxmaj he was a dreamer when younger , and unlike many he did n't change when he grew up . \\n\\n xxmaj about transitions , they all are very very smooth , and you</td>\n    </tr>\n  </tbody>\n</table>"},"metadata":{}}]},{"cell_type":"markdown","source":"#### Fine-tuning the language model\n\nTo convert the integer word indices into activations that we can use for our neural network, we will use embeddings. \n\nThen we'll feed those embeddings into a recurrent neural network (RNN), using an architecture called AWD-LSTM.","metadata":{"id":"m-vwhvDXYbMN"}},{"cell_type":"code","source":"learner = language_model_learner(\n    dls_lm,\n    AWD_LSTM,\n    drop_mult=0.3, # for regularization\n    metrics=[accuracy, Perplexity()]\n).to_fp16()","metadata":{"id":"7Q_fWnO4YbMN","execution":{"iopub.status.busy":"2023-06-04T18:28:39.472270Z","iopub.execute_input":"2023-06-04T18:28:39.472627Z","iopub.status.idle":"2023-06-04T18:28:45.352110Z","shell.execute_reply.started":"2023-06-04T18:28:39.472598Z","shell.execute_reply":"2023-06-04T18:28:45.351041Z"},"trusted":true},"execution_count":21,"outputs":[{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n<style>\n    /* Turns off some styling */\n    progress {\n        /* gets rid of default border in Firefox and Opera. */\n        border: none;\n        /* Needs to be in here for Safari polyfill so background images work as expected. */\n        background-size: auto;\n    }\n    progress:not([value]), progress:not([value])::-webkit-progress-bar {\n        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);\n    }\n    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n        background: #F44336;\n    }\n</style>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      <progress value='105070592' class='' max='105067061' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      100.00% [105070592/105067061 00:01&lt;00:00]\n    </div>\n    "},"metadata":{}}]},{"cell_type":"markdown","source":"The loss function used by default is cross-entropy loss, since we essentially have a classification problem (the different categories being the words in our vocab). The perplexity metric used here is often used in NLP for language models: it is the exponential of the loss (i.e., `torch.exp(cross_entropy)`).","metadata":{"id":"HB9K8LACYbMN"}},{"cell_type":"code","source":"?learner.fit_one_cycle","metadata":{"id":"OutXGlZWYbMO","execution":{"iopub.status.busy":"2023-06-04T18:28:51.052200Z","iopub.execute_input":"2023-06-04T18:28:51.052565Z","iopub.status.idle":"2023-06-04T18:28:51.148101Z","shell.execute_reply.started":"2023-06-04T18:28:51.052535Z","shell.execute_reply":"2023-06-04T18:28:51.147175Z"},"trusted":true},"execution_count":22,"outputs":[{"output_type":"display_data","data":{"text/plain":"\u001b[0;31mSignature:\u001b[0m\n\u001b[0mlearner\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_one_cycle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\n\u001b[0;34m\u001b[0m    \u001b[0mn_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n\u001b[0;34m\u001b[0m    \u001b[0mlr_max\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n\u001b[0;34m\u001b[0m    \u001b[0mdiv\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m25.0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n\u001b[0;34m\u001b[0m    \u001b[0mdiv_final\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100000.0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n\u001b[0;34m\u001b[0m    \u001b[0mpct_start\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.25\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n\u001b[0;34m\u001b[0m    \u001b[0mwd\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n\u001b[0;34m\u001b[0m    \u001b[0mmoms\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n\u001b[0;34m\u001b[0m    \u001b[0mcbs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n\u001b[0;34m\u001b[0m    \u001b[0mreset_opt\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n\u001b[0;34m\u001b[0m    \u001b[0mstart_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n\u001b[0;34m\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;31mDocstring:\u001b[0m Fit `self.model` for `n_epoch` using the 1cycle policy.\n\u001b[0;31mFile:\u001b[0m      /opt/conda/lib/python3.10/site-packages/fastai/callback/schedule.py\n\u001b[0;31mType:\u001b[0m      method"},"metadata":{}}]},{"cell_type":"code","source":"\"\"\"language_model_learner automatically calls freeze when using a pretrained model \n(which is the default), so this will only train the embeddings (the only part of the model \nthat contains randomly initialized weights—i.e., embeddings for words that are in our IMDb vocab, \nbut aren't in the pretrained model vocab).\n\"\"\"\n\n# execute on Colab or Paper Space\nlearner.fit_one_cycle(n_epoch=1, lr_max=2e-2)","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":378},"id":"Jj7h36URYbMO","outputId":"f6a21610-c463-4bd3-aacd-7ccd0ff42c35","execution":{"iopub.status.busy":"2023-06-04T18:28:56.027452Z","iopub.execute_input":"2023-06-04T18:28:56.027812Z","iopub.status.idle":"2023-06-04T18:57:56.295962Z","shell.execute_reply.started":"2023-06-04T18:28:56.027785Z","shell.execute_reply":"2023-06-04T18:57:56.294724Z"},"trusted":true},"execution_count":23,"outputs":[{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n<style>\n    /* Turns off some styling */\n    progress {\n        /* gets rid of default border in Firefox and Opera. */\n        border: none;\n        /* Needs to be in here for Safari polyfill so background images work as expected. */\n        background-size: auto;\n    }\n    progress:not([value]), progress:not([value])::-webkit-progress-bar {\n        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);\n    }\n    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n        background: #F44336;\n    }\n</style>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: left;\">\n      <th>epoch</th>\n      <th>train_loss</th>\n      <th>valid_loss</th>\n      <th>accuracy</th>\n      <th>perplexity</th>\n      <th>time</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>0</td>\n      <td>4.010259</td>\n      <td>3.900625</td>\n      <td>0.300411</td>\n      <td>49.433323</td>\n      <td>29:00</td>\n    </tr>\n  </tbody>\n</table>"},"metadata":{}}]},{"cell_type":"markdown","source":"#### Saving and Loading Models","metadata":{"id":"k73_4Ur-arZc"}},{"cell_type":"code","source":"# save weights\nlearner.save(\"1epoch\")\n# load weights\nlearner = learner.load(\"1epoch\")","metadata":{"id":"0xwsSUXgavOh","execution":{"iopub.status.busy":"2023-06-04T19:02:06.321299Z","iopub.execute_input":"2023-06-04T19:02:06.321699Z","iopub.status.idle":"2023-06-04T19:02:07.356134Z","shell.execute_reply.started":"2023-06-04T19:02:06.321663Z","shell.execute_reply":"2023-06-04T19:02:07.355080Z"},"trusted":true},"execution_count":24,"outputs":[]},{"cell_type":"code","source":"!ls $learner.path/models ","metadata":{"execution":{"iopub.status.busy":"2023-06-04T19:03:32.812531Z","iopub.execute_input":"2023-06-04T19:03:32.813608Z","iopub.status.idle":"2023-06-04T19:03:33.848366Z","shell.execute_reply.started":"2023-06-04T19:03:32.813569Z","shell.execute_reply":"2023-06-04T19:03:33.847145Z"},"trusted":true},"execution_count":28,"outputs":[{"name":"stdout","text":"1epoch.pth\n","output_type":"stream"}]},{"cell_type":"code","source":"!cp -R $learner.path/models /kaggle/output","metadata":{"execution":{"iopub.status.busy":"2023-06-04T19:08:28.654116Z","iopub.execute_input":"2023-06-04T19:08:28.654510Z","iopub.status.idle":"2023-06-04T19:08:30.036605Z","shell.execute_reply.started":"2023-06-04T19:08:28.654478Z","shell.execute_reply":"2023-06-04T19:08:30.035190Z"},"trusted":true},"execution_count":31,"outputs":[]},{"cell_type":"code","source":"# once the initial training has completed, we can continue fine-tuning the model\n# after unfreezing\n\nlearner.unfreeze()\nlearner.fit_one_cycle(n_epoch=7, lr_max=2e-2)","metadata":{"id":"y8bLf3Wka8Jd","execution":{"iopub.status.busy":"2023-06-04T19:08:59.607858Z","iopub.execute_input":"2023-06-04T19:08:59.608262Z","iopub.status.idle":"2023-06-04T22:44:02.874153Z","shell.execute_reply.started":"2023-06-04T19:08:59.608228Z","shell.execute_reply":"2023-06-04T22:44:02.872873Z"},"trusted":true},"execution_count":32,"outputs":[{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n<style>\n    /* Turns off some styling */\n    progress {\n        /* gets rid of default border in Firefox and Opera. */\n        border: none;\n        /* Needs to be in here for Safari polyfill so background images work as expected. */\n        background-size: auto;\n    }\n    progress:not([value]), progress:not([value])::-webkit-progress-bar {\n        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);\n    }\n    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n        background: #F44336;\n    }\n</style>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: left;\">\n      <th>epoch</th>\n      <th>train_loss</th>\n      <th>valid_loss</th>\n      <th>accuracy</th>\n      <th>perplexity</th>\n      <th>time</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>0</td>\n      <td>3.987087</td>\n      <td>3.975214</td>\n      <td>0.296409</td>\n      <td>53.261490</td>\n      <td>29:53</td>\n    </tr>\n    <tr>\n      <td>1</td>\n      <td>4.106718</td>\n      <td>4.063072</td>\n      <td>0.289464</td>\n      <td>58.152668</td>\n      <td>30:13</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>3.986650</td>\n      <td>3.952163</td>\n      <td>0.300890</td>\n      <td>52.047848</td>\n      <td>31:22</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>3.863941</td>\n      <td>3.835424</td>\n      <td>0.311763</td>\n      <td>46.313080</td>\n      <td>30:50</td>\n    </tr>\n    <tr>\n      <td>4</td>\n      <td>3.697453</td>\n      <td>3.709002</td>\n      <td>0.324382</td>\n      <td>40.813057</td>\n      <td>30:45</td>\n    </tr>\n    <tr>\n      <td>5</td>\n      <td>3.548256</td>\n      <td>3.620828</td>\n      <td>0.334130</td>\n      <td>37.368492</td>\n      <td>30:51</td>\n    </tr>\n    <tr>\n      <td>6</td>\n      <td>3.430043</td>\n      <td>3.606477</td>\n      <td>0.336346</td>\n      <td>36.836044</td>\n      <td>31:07</td>\n    </tr>\n  </tbody>\n</table>"},"metadata":{}}]},{"cell_type":"markdown","source":"Once this is done, we save all of our model except the final layer that converts activations to probabilities of picking each token in our vocabulary. The model not including the final layer is called the encoder. We can save it with save_encoder:","metadata":{"id":"o0sgwVG6cOW5"}},{"cell_type":"code","source":"learner.save_encoder(\"finetuned\")","metadata":{"id":"YhkffyfVcPuV","execution":{"iopub.status.busy":"2023-06-04T22:46:30.315615Z","iopub.execute_input":"2023-06-04T22:46:30.316595Z","iopub.status.idle":"2023-06-04T22:46:30.639323Z","shell.execute_reply.started":"2023-06-04T22:46:30.316558Z","shell.execute_reply":"2023-06-04T22:46:30.638239Z"},"trusted":true},"execution_count":33,"outputs":[]},{"cell_type":"code","source":"!ls $learner.path/models ","metadata":{"execution":{"iopub.status.busy":"2023-06-04T22:46:44.355714Z","iopub.execute_input":"2023-06-04T22:46:44.356666Z","iopub.status.idle":"2023-06-04T22:46:45.477605Z","shell.execute_reply.started":"2023-06-04T22:46:44.356612Z","shell.execute_reply":"2023-06-04T22:46:45.476399Z"},"trusted":true},"execution_count":34,"outputs":[{"name":"stdout","text":"1epoch.pth  finetuned.pth\n","output_type":"stream"}]},{"cell_type":"code","source":"!cp -R $learner.path/models /kaggle/output","metadata":{"execution":{"iopub.status.busy":"2023-06-04T22:48:43.955837Z","iopub.execute_input":"2023-06-04T22:48:43.956841Z","iopub.status.idle":"2023-06-04T22:48:45.496790Z","shell.execute_reply.started":"2023-06-04T22:48:43.956802Z","shell.execute_reply":"2023-06-04T22:48:45.495477Z"},"trusted":true},"execution_count":35,"outputs":[]},{"cell_type":"markdown","source":"### Text generation","metadata":{}},{"cell_type":"code","source":"# we will try the model to generate text, because at this point, the model is trained to guess\n# what the next word of the sentence is, we can use the model to write new reviews\n\nPROMPT = \"I like SpiderMan because\"\nN_WORDS = 40\nN_SENTENCES = 1\n\npreds = [learner.predict(PROMPT, N_WORDS, temperature=0.5) for _ in range(N_SENTENCES)]\n#print(\"\\n\".join(preds))","metadata":{"execution":{"iopub.status.busy":"2023-06-04T22:58:17.833800Z","iopub.execute_input":"2023-06-04T22:58:17.836799Z","iopub.status.idle":"2023-06-04T22:58:25.148412Z","shell.execute_reply.started":"2023-06-04T22:58:17.836747Z","shell.execute_reply":"2023-06-04T22:58:25.147474Z"},"trusted":true},"execution_count":52,"outputs":[{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n<style>\n    /* Turns off some styling */\n    progress {\n        /* gets rid of default border in Firefox and Opera. */\n        border: none;\n        /* Needs to be in here for Safari polyfill so background images work as expected. */\n        background-size: auto;\n    }\n    progress:not([value]), progress:not([value])::-webkit-progress-bar {\n        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);\n    }\n    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n        background: #F44336;\n    }\n</style>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":""},"metadata":{}}]},{"cell_type":"code","source":"for idx, stream in enumerate(preds[0].split(\" \")):\n    print(stream, end=\" \")\n    \n    if idx > 0 and idx % 8 == 0:\n        print(\"\")","metadata":{"execution":{"iopub.status.busy":"2023-06-04T22:58:28.920638Z","iopub.execute_input":"2023-06-04T22:58:28.921029Z","iopub.status.idle":"2023-06-04T22:58:28.927124Z","shell.execute_reply.started":"2023-06-04T22:58:28.921000Z","shell.execute_reply":"2023-06-04T22:58:28.926184Z"},"trusted":true},"execution_count":53,"outputs":[{"name":"stdout","text":"i like spiderman because it 's a good movie \n. But that 's not the point . \nThis movie is about a robot who is \ntrying to create a robot . As it \nturns out , he 's a super hero \n","output_type":"stream"}]},{"cell_type":"markdown","source":"### Creating the classifier\n\nWe're now moving from language model fine-tuning to classifier fine-tuning. To recap, a language model predicts the next word of a document, so it doesn't need any external labels. A classifier, however, predicts some external label—in the case of IMDb, it's the sentiment of a document.","metadata":{}},{"cell_type":"code","source":"from fastai.text.all import (\n    CategoryBlock,\n    GrandparentSplitter,\n    parent_label,\n)","metadata":{"execution":{"iopub.status.busy":"2023-06-04T23:18:34.330838Z","iopub.execute_input":"2023-06-04T23:18:34.331220Z","iopub.status.idle":"2023-06-04T23:18:34.336326Z","shell.execute_reply.started":"2023-06-04T23:18:34.331189Z","shell.execute_reply":"2023-06-04T23:18:34.335387Z"},"trusted":true},"execution_count":56,"outputs":[]},{"cell_type":"code","source":"batch_size = 128\nseq_len = 72\n\n# The reason that we pass the vocab of the language model is to make sure we use \n# the same correspondence of token to index. Otherwise the embeddings we learned in our \n# fine-tuned language model won't make any sense to this model, and the fine-tuning \n# step won't be of any use.\ndls_class = DataBlock(\n    blocks=(TextBlock.from_folder(path, vocab=dls_lm.vocab), CategoryBlock),\n    get_y=parent_label,\n    get_items=partial(get_text_files, folders=[\"train\", \"test\"]),\n    splitter=GrandparentSplitter(valid_name=\"test\")\n).dataloaders(path, path=path, bs=batch_size, seq_len=seq_len)","metadata":{"execution":{"iopub.status.busy":"2023-06-04T23:18:36.427712Z","iopub.execute_input":"2023-06-04T23:18:36.428088Z","iopub.status.idle":"2023-06-04T23:18:43.214821Z","shell.execute_reply.started":"2023-06-04T23:18:36.428060Z","shell.execute_reply":"2023-06-04T23:18:43.213609Z"},"trusted":true},"execution_count":57,"outputs":[]},{"cell_type":"code","source":"dls_class.show_batch(max_n=2)","metadata":{"execution":{"iopub.status.busy":"2023-06-04T23:19:00.028576Z","iopub.execute_input":"2023-06-04T23:19:00.029577Z","iopub.status.idle":"2023-06-04T23:19:00.568849Z","shell.execute_reply.started":"2023-06-04T23:19:00.029535Z","shell.execute_reply":"2023-06-04T23:19:00.563731Z"},"trusted":true},"execution_count":59,"outputs":[{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>text</th>\n      <th>category</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>xxbos xxmaj match 1 : xxmaj tag xxmaj team xxmaj table xxmaj match xxmaj bubba xxmaj ray and xxmaj spike xxmaj dudley vs xxmaj eddie xxmaj guerrero and xxmaj chris xxmaj benoit xxmaj bubba xxmaj ray and xxmaj spike xxmaj dudley started things off with a xxmaj tag xxmaj team xxmaj table xxmaj match against xxmaj eddie xxmaj guerrero and xxmaj chris xxmaj benoit . xxmaj according to the rules of the match , both opponents have to go through tables in order to get the win . xxmaj benoit and xxmaj guerrero heated up early on by taking turns hammering first xxmaj spike and then xxmaj bubba xxmaj ray . a xxmaj german xxunk by xxmaj benoit to xxmaj bubba took the wind out of the xxmaj dudley brother . xxmaj spike tried to help his brother , but the referee restrained him while xxmaj benoit and xxmaj guerrero</td>\n      <td>pos</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>xxbos * ! ! - xxup spoilers - ! ! * \\n\\n xxmaj before i begin this , let me say that i have had both the advantages of seeing this movie on the big screen and of having seen the \" authorized xxmaj version \" of this movie , remade by xxmaj stephen xxmaj king , himself , in 1997 . \\n\\n xxmaj both advantages made me appreciate this version of \" the xxmaj shining , \" all the more . \\n\\n xxmaj also , let me say that xxmaj i 've read xxmaj mr . xxmaj king 's book , \" the xxmaj shining \" on many occasions over the years , and while i love the book and am a huge fan of his work , xxmaj stanley xxmaj kubrick 's retelling of this story is far more compelling … and xxup scary . \\n\\n xxmaj kubrick</td>\n      <td>pos</td>\n    </tr>\n  </tbody>\n</table>"},"metadata":{}}]},{"cell_type":"markdown","source":"There is one challenge we have to deal with, however, which is to do with collating multiple documents into a mini-batch. Let's see with an example, by trying to create a mini-batch containing the first 10 documents.","metadata":{}},{"cell_type":"code","source":"nums_samp = toks200[:10].map(numericalizer)\n# we notice that each review has a different amount of tokens\nnums_samp.map(len)","metadata":{"execution":{"iopub.status.busy":"2023-06-04T23:21:18.320676Z","iopub.execute_input":"2023-06-04T23:21:18.321806Z","iopub.status.idle":"2023-06-04T23:21:18.335314Z","shell.execute_reply.started":"2023-06-04T23:21:18.321752Z","shell.execute_reply":"2023-06-04T23:21:18.334195Z"},"trusted":true},"execution_count":62,"outputs":[{"execution_count":62,"output_type":"execute_result","data":{"text/plain":"(#10) [158,319,181,193,114,145,260,146,252,295]"},"metadata":{}}]},{"cell_type":"markdown","source":" PyTorch DataLoaders need to collate all the items in a batch into a single tensor, and a single tensor has a fixed shape.\n \n We will expand the shortest texts to make them all the same size. To do this, we use a special padding token that will be ignored by our model. \n \nAdditionally, to avoid memory issues and improve performance, we will batch together texts that are roughly the same lengths (with some shuffling for the training set). We do this by (approximately, for the training set) sorting the documents by length prior to each epoch. \n\nThe result of this is that the documents collated into a single batch will tend to be of similar lengths. We won't pad every batch to the same size, but will instead use the size of the largest document in each batch as the target size. \n\nThe sorting and padding are automatically done by the data block API for us when using a TextBlock, with `is_lm=False`.","metadata":{}},{"cell_type":"code","source":"from fastai.text.all import (\n    text_classifier_learner,\n)","metadata":{"execution":{"iopub.status.busy":"2023-06-04T23:36:03.060485Z","iopub.execute_input":"2023-06-04T23:36:03.061618Z","iopub.status.idle":"2023-06-04T23:36:03.066263Z","shell.execute_reply.started":"2023-06-04T23:36:03.061574Z","shell.execute_reply":"2023-06-04T23:36:03.065236Z"},"trusted":true},"execution_count":70,"outputs":[]},{"cell_type":"code","source":"# we can now create a model to classify our texts\n\nclassifier = text_classifier_learner(\n    dls_class, AWD_LSTM, drop_mult=0.5, metrics=accuracy\n).to_fp16()","metadata":{"execution":{"iopub.status.busy":"2023-06-04T23:22:02.454747Z","iopub.execute_input":"2023-06-04T23:22:02.455110Z","iopub.status.idle":"2023-06-04T23:22:04.210151Z","shell.execute_reply.started":"2023-06-04T23:22:02.455081Z","shell.execute_reply":"2023-06-04T23:22:04.209070Z"},"trusted":true},"execution_count":65,"outputs":[]},{"cell_type":"code","source":"# The final step prior to training the classifier is to load the encoder from our \n# fine-tuned language model. We use load_encoder instead of load because we \n# only have pretrained weights available for the encoder\n\nclassifier = classifier.load_encoder(\"finetuned\")","metadata":{"execution":{"iopub.status.busy":"2023-06-04T23:22:09.964441Z","iopub.execute_input":"2023-06-04T23:22:09.964817Z","iopub.status.idle":"2023-06-04T23:22:10.151253Z","shell.execute_reply.started":"2023-06-04T23:22:09.964785Z","shell.execute_reply":"2023-06-04T23:22:10.150332Z"},"trusted":true},"execution_count":66,"outputs":[]},{"cell_type":"markdown","source":"### Fine-Tuning the Classifier\n\nThe last step is to train with discriminative learning rates and gradual unfreezing. In computer vision we often unfreeze the model all at once, but for NLP classifiers, we find that unfreezing a few layers at a time makes a real difference.\n\nDiscriminative learning rate is one of the tricks that can help us guide fine-tuning. By using lower learning rates on deeper layers of the network, we make sure we are not tempering too much with the model blocks that have already learned general patterns and concentrate fine-tuning on further layers","metadata":{}},{"cell_type":"code","source":"classifier.fit_one_cycle(1, 2e-2)","metadata":{"execution":{"iopub.status.busy":"2023-06-04T23:32:32.510488Z","iopub.execute_input":"2023-06-04T23:32:32.511178Z","iopub.status.idle":"2023-06-04T23:33:43.225080Z","shell.execute_reply.started":"2023-06-04T23:32:32.511144Z","shell.execute_reply":"2023-06-04T23:33:43.223915Z"},"trusted":true},"execution_count":67,"outputs":[{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n<style>\n    /* Turns off some styling */\n    progress {\n        /* gets rid of default border in Firefox and Opera. */\n        border: none;\n        /* Needs to be in here for Safari polyfill so background images work as expected. */\n        background-size: auto;\n    }\n    progress:not([value]), progress:not([value])::-webkit-progress-bar {\n        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);\n    }\n    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n        background: #F44336;\n    }\n</style>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: left;\">\n      <th>epoch</th>\n      <th>train_loss</th>\n      <th>valid_loss</th>\n      <th>accuracy</th>\n      <th>time</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>0</td>\n      <td>0.242175</td>\n      <td>0.181831</td>\n      <td>0.930160</td>\n      <td>01:10</td>\n    </tr>\n  </tbody>\n</table>"},"metadata":{}}]},{"cell_type":"code","source":"?classifier.fit_one_cycle","metadata":{"execution":{"iopub.status.busy":"2023-06-04T23:34:22.193466Z","iopub.execute_input":"2023-06-04T23:34:22.193880Z","iopub.status.idle":"2023-06-04T23:34:22.203933Z","shell.execute_reply.started":"2023-06-04T23:34:22.193845Z","shell.execute_reply":"2023-06-04T23:34:22.202864Z"},"trusted":true},"execution_count":68,"outputs":[{"output_type":"display_data","data":{"text/plain":"\u001b[0;31mSignature:\u001b[0m\n\u001b[0mclassifier\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_one_cycle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\n\u001b[0;34m\u001b[0m    \u001b[0mn_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n\u001b[0;34m\u001b[0m    \u001b[0mlr_max\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n\u001b[0;34m\u001b[0m    \u001b[0mdiv\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m25.0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n\u001b[0;34m\u001b[0m    \u001b[0mdiv_final\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100000.0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n\u001b[0;34m\u001b[0m    \u001b[0mpct_start\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.25\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n\u001b[0;34m\u001b[0m    \u001b[0mwd\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n\u001b[0;34m\u001b[0m    \u001b[0mmoms\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n\u001b[0;34m\u001b[0m    \u001b[0mcbs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n\u001b[0;34m\u001b[0m    \u001b[0mreset_opt\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n\u001b[0;34m\u001b[0m    \u001b[0mstart_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n\u001b[0;34m\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;31mDocstring:\u001b[0m Fit `self.model` for `n_epoch` using the 1cycle policy.\n\u001b[0;31mFile:\u001b[0m      /opt/conda/lib/python3.10/site-packages/fastai/callback/schedule.py\n\u001b[0;31mType:\u001b[0m      method"},"metadata":{}}]},{"cell_type":"code","source":"?slice","metadata":{"execution":{"iopub.status.busy":"2023-06-04T23:36:08.792114Z","iopub.execute_input":"2023-06-04T23:36:08.792550Z","iopub.status.idle":"2023-06-04T23:36:08.804635Z","shell.execute_reply.started":"2023-06-04T23:36:08.792518Z","shell.execute_reply":"2023-06-04T23:36:08.803688Z"},"trusted":true},"execution_count":71,"outputs":[{"output_type":"display_data","data":{"text/plain":"\u001b[0;31mInit signature:\u001b[0m \u001b[0mslice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m/\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;31mDocstring:\u001b[0m     \nslice(stop)\nslice(start, stop[, step])\n\nCreate a slice object.  This is used for extended slicing (e.g. a[0:10:2]).\n\u001b[0;31mType:\u001b[0m           type\n\u001b[0;31mSubclasses:\u001b[0m     "},"metadata":{}}]},{"cell_type":"code","source":"lr = slice(1e-2/(2.6**4),1e-2)\n\"\"\"A slice object is used to specify how to slice a sequence. You can specify\nwhere to start the slicing, and where to end. You can also specify the step.\n\"\"\"\nprint(type(lr))\nlr","metadata":{"execution":{"iopub.status.busy":"2023-06-04T23:36:42.710131Z","iopub.execute_input":"2023-06-04T23:36:42.710520Z","iopub.status.idle":"2023-06-04T23:36:42.720772Z","shell.execute_reply.started":"2023-06-04T23:36:42.710486Z","shell.execute_reply":"2023-06-04T23:36:42.719463Z"},"trusted":true},"execution_count":73,"outputs":[{"name":"stdout","text":"<class 'slice'>\n","output_type":"stream"},{"execution_count":73,"output_type":"execute_result","data":{"text/plain":"slice(0.00021882987290360977, 0.01, None)"},"metadata":{}}]},{"cell_type":"code","source":"# We can pass -2 to freeze_to to freeze all except the last two parameter groups\n\nclassifier.freeze_to(-2)\nclassifier.fit_one_cycle(n_epoch=1, lr_max=lr)","metadata":{"execution":{"iopub.status.busy":"2023-06-04T23:38:59.192816Z","iopub.execute_input":"2023-06-04T23:38:59.193798Z","iopub.status.idle":"2023-06-04T23:40:13.662113Z","shell.execute_reply.started":"2023-06-04T23:38:59.193749Z","shell.execute_reply":"2023-06-04T23:40:13.661048Z"},"trusted":true},"execution_count":75,"outputs":[{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n<style>\n    /* Turns off some styling */\n    progress {\n        /* gets rid of default border in Firefox and Opera. */\n        border: none;\n        /* Needs to be in here for Safari polyfill so background images work as expected. */\n        background-size: auto;\n    }\n    progress:not([value]), progress:not([value])::-webkit-progress-bar {\n        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);\n    }\n    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n        background: #F44336;\n    }\n</style>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: left;\">\n      <th>epoch</th>\n      <th>train_loss</th>\n      <th>valid_loss</th>\n      <th>accuracy</th>\n      <th>time</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>0</td>\n      <td>0.212653</td>\n      <td>0.167675</td>\n      <td>0.936000</td>\n      <td>01:14</td>\n    </tr>\n  </tbody>\n</table>"},"metadata":{}}]},{"cell_type":"code","source":"lr = slice(5e-3/(2.6**4), 5e-3)\n\nclassifier.freeze_to(-3)\nclassifier.fit_one_cycle(n_epoch=1, lr_max=lr)","metadata":{"execution":{"iopub.status.busy":"2023-06-04T23:43:28.821348Z","iopub.execute_input":"2023-06-04T23:43:28.821854Z","iopub.status.idle":"2023-06-04T23:44:57.431176Z","shell.execute_reply.started":"2023-06-04T23:43:28.821813Z","shell.execute_reply":"2023-06-04T23:44:57.430013Z"},"trusted":true},"execution_count":77,"outputs":[{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n<style>\n    /* Turns off some styling */\n    progress {\n        /* gets rid of default border in Firefox and Opera. */\n        border: none;\n        /* Needs to be in here for Safari polyfill so background images work as expected. */\n        background-size: auto;\n    }\n    progress:not([value]), progress:not([value])::-webkit-progress-bar {\n        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);\n    }\n    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n        background: #F44336;\n    }\n</style>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: left;\">\n      <th>epoch</th>\n      <th>train_loss</th>\n      <th>valid_loss</th>\n      <th>accuracy</th>\n      <th>time</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>0</td>\n      <td>0.172222</td>\n      <td>0.157002</td>\n      <td>0.941800</td>\n      <td>01:28</td>\n    </tr>\n  </tbody>\n</table>"},"metadata":{}}]},{"cell_type":"code","source":"# unfreeze the whole model\nclassifier.unfreeze()\nclassifier.fit_one_cycle(2, slice(1e-3/(2.6**4), 1e-3))","metadata":{"execution":{"iopub.status.busy":"2023-06-04T23:50:05.563610Z","iopub.execute_input":"2023-06-04T23:50:05.564065Z","iopub.status.idle":"2023-06-04T23:53:35.321145Z","shell.execute_reply.started":"2023-06-04T23:50:05.564030Z","shell.execute_reply":"2023-06-04T23:53:35.319960Z"},"trusted":true},"execution_count":78,"outputs":[{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n<style>\n    /* Turns off some styling */\n    progress {\n        /* gets rid of default border in Firefox and Opera. */\n        border: none;\n        /* Needs to be in here for Safari polyfill so background images work as expected. */\n        background-size: auto;\n    }\n    progress:not([value]), progress:not([value])::-webkit-progress-bar {\n        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);\n    }\n    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n        background: #F44336;\n    }\n</style>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: left;\">\n      <th>epoch</th>\n      <th>train_loss</th>\n      <th>valid_loss</th>\n      <th>accuracy</th>\n      <th>time</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>0</td>\n      <td>0.145825</td>\n      <td>0.158671</td>\n      <td>0.942320</td>\n      <td>01:43</td>\n    </tr>\n    <tr>\n      <td>1</td>\n      <td>0.138071</td>\n      <td>0.158919</td>\n      <td>0.942280</td>\n      <td>01:46</td>\n    </tr>\n  </tbody>\n</table>"},"metadata":{}}]}]}