{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What is tokenization?\n",
    "\n",
    "\n",
    "From natural languaje text (or code), we take words, o subwords and then map to integers to train and feed machine learning models like neural networks. But it's not a simple process, what do we do with puntuation and more subtle things like special characters and contractions (in English)?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Approaches\n",
    "\n",
    "- Word based: Split a sentence on spaces, as well as applying language-specific rules to try to separate parts of meaning even when there are no spaces (such as turning \"don't\" into \"do n't\"). Generally, punctuation marks are also split into separate tokens.\n",
    "\n",
    "- Subword based: Split words into smaller parts, based on the most commonly occurring substrings. For instance, \"occasion\" might be tokenized as \"o c ca sion.\"\n",
    "\n",
    "- Character-based: Split a sentence into its individual characters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastai.text.all import (\n",
    "    coll_repr,\n",
    "    defaults,\n",
    "    first,\n",
    "    get_text_files,\n",
    "    L,\n",
    "    Numericalize,\n",
    "    Tokenizer,\n",
    "    SubwordTokenizer,\n",
    "    untar_data,\n",
    "    URLs,\n",
    "    WordTokenizer,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word Tokenization with FastAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we will use text from IMDB movie reviews\n",
    "\n",
    "path = untar_data(URLs.IMDB)\n",
    "files = get_text_files(path, folders = ['train', 'test', 'unsup'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data was downloaded on /home/david/.fastai/data/imdb\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Absolutely the worst piece of crap my brother and I have seen. The movie lo'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Data was downloaded on\", path)\n",
    "# check a text from the first file containing reviews\n",
    "example_txt = files[0].open().read()[:75]\n",
    "example_txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"(#16) ['Absolutely','the','worst','piece','of','crap','my','brother','and','I','have','seen','.','The','movie','lo']\""
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We use a English word tokenizer\n",
    "spacy = WordTokenizer()\n",
    "\n",
    "tokens = first(spacy([example_txt]))\n",
    "\n",
    "# this function only print n elements from a collection, and it displays the full lenght\n",
    "coll_repr(tokens, max_n=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"(#19) ['xxbos','xxmaj','absolutely','the','worst','piece','of','crap','my','brother','and','i','have','seen','.','xxmaj','the','movie','lo']\""
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Add special tokens with fastai's Tokenizer class\n",
    "tkn = Tokenizer(spacy)\n",
    "\n",
    "coll_repr(tkn(example_txt), max_n=31)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some of those special tokens are:\n",
    "\n",
    "- **xxbos**: Indicates the beginning of a text (here, a review)\n",
    "- **xxmaj**: Indicates the next word begins with a capital (since we lowercased everything)\n",
    "- **xxunk**: Indicates the word is unknown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<function fastai.text.core.fix_html(x)>,\n",
       " <function fastai.text.core.replace_rep(t)>,\n",
       " <function fastai.text.core.replace_wrep(t)>,\n",
       " <function fastai.text.core.spec_add_spaces(t)>,\n",
       " <function fastai.text.core.rm_useless_spaces(t)>,\n",
       " <function fastai.text.core.replace_all_caps(t)>,\n",
       " <function fastai.text.core.replace_maj(t)>,\n",
       " <function fastai.text.core.lowercase(t, add_bos=True, add_eos=False)>]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# actually we can see the rules applied during fastai's tokenization\n",
    "defaults.text_proc_rules"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is a brief summary of what each does:\n",
    "\n",
    "- **fix_html**: Replaces special HTML characters with a readable version.\n",
    "- **replace_rep**: Replaces any character repeated three times or more with a special token for repetition (xxrep), the number of repetitions then the character\n",
    "- **replace_wrep**: Replaces any word repeated three times or more with a special token for word repetition (xxwrep), the number of repetitions, then the word\n",
    "- **spec_add_spaces**: Adds spaces around / and #\n",
    "- **rm_useless_spaces**: Removes all repetitions of the space character\n",
    "- **replace_all_caps**: Lowercases a word written in all caps and adds a special token for all caps (xxup) in front of it\n",
    "- **replace_maj**: Lowercases a capitalized word and adds a special token for capitalized (xxmaj) in front of it\n",
    "- **lowercase**: Lowercases all text and adds a special token at the beginning (xxbos) and/or the end (xxeos)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Subword tokenization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In some languages the concept of space is not so clear, or event don't exist at all (example, Chinese).\n",
    "The approach of subword tokenization proceeds in two steps:\n",
    "1) Analyze a corpus of documents to find the most commonly occurring groups of letters. These become the vocab.\n",
    "2) Tokenize the corpus using this vocab of subword units."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to demostrate this method we will select a corpus of 2000 movie reviews\n",
    "txts = L(o.open().read() for o in files[:2000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def subword(vocab_size: int):\n",
    "    sp = SubwordTokenizer(vocab_sz=vocab_size)\n",
    "    sp.setup(txts)\n",
    "\n",
    "    # return the first 40 tokens as demostration\n",
    "    return \" \".join(first(sp([txts]))[:40])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       "    /* Turns off some styling */\n",
       "    progress {\n",
       "        /* gets rid of default border in Firefox and Opera. */\n",
       "        border: none;\n",
       "        /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "        background-size: auto;\n",
       "    }\n",
       "    progress:not([value]), progress:not([value])::-webkit-progress-bar {\n",
       "        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);\n",
       "    }\n",
       "    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "        background: #F44336;\n",
       "    }\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "subword(150) # this crashes the kernel :("
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Numericalization with fastai"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It the process of mapping tokens to integers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = tkn(example_txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"(#19) ['xxbos','xxmaj','absolutely','the','worst','piece','of','crap','my','brother','and','i','have','seen','.','xxmaj','the','movie','lo']\""
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "coll_repr(tkn(example_txt), max_n=31)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(#167) ['xxbos','xxmaj','absolutely','the','worst','piece','of','crap','my','brother'...]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Just like SubwordTokenizer we need to call setup on Numeralize\n",
    "toks200 = txts[:200].map(tkn)\n",
    "toks200[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"(#2136) ['xxunk','xxpad','xxbos','xxeos','xxfld','xxrep','xxwrep','xxup','xxmaj','the',',','.','and','a','of','to','is','it','in','this'...]\""
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "numericalizer = Numericalize()\n",
    "\n",
    "numericalizer.setup(toks200)\n",
    "coll_repr(numericalizer.vocab, 20)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The special rules tokens appear first, and then every word appears once, in frequency order. The defaults to `Numericalize` are `min_freq=3`, `max_vocab=60000`. `max_vocab=60000` results in fastai replacing all words other than the most common 60,000 with a special unknown word token, xxunk. This is useful to avoid having an overly large embedding matrix, since that can slow down training and use up too much memory, and can also mean that there isn't enough data to train useful representations for rare words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorText([  2,   8, 495,   9, 237, 452,  14, 879,  89, 666,  12,  20,  43,\n",
       "            155,  11,   8,   9,  27,   0])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# we can use out numericalizer to get the integers from text tokens\n",
    "\n",
    "numericalizer(tokens)[:20]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
