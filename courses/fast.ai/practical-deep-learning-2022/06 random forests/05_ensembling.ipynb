{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ensembling\n",
    "\n",
    "Think back to the original reasoning behind why random forests work so well: each tree has errors, but those errors are not correlated with each other, so the average of those errors should tend towards zero once there are enough trees.\n",
    "\n",
    "Similar reasoning could be used to consider averaging the predictions of models trained using different algorithms (for example, a random forest and a neural network)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from fastai.tabular.all import (\n",
    "    Categorify,\n",
    "    FillMissing,\n",
    "    Normalize,\n",
    "    F,\n",
    "    TabularPandas,\n",
    "    add_datepart,\n",
    "    cont_cat_split,\n",
    "    tabular_learner,\n",
    "    to_np,\n",
    ")\n",
    "from fastbook import (\n",
    "    Path,\n",
    "    load_pickle,\n",
    ")\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "from evaluation import m_rmse, r_mse"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading data and setup - Random forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = Path('/home/david/.fastai/archive/bluebook-for-bulldozers')\n",
    "Path.BASE_PATH = path\n",
    "\n",
    "to = load_pickle(path / \"to.pkl\")\n",
    "\n",
    "xs_final = load_pickle(path / \"xs_final.pkl\")\n",
    "y = to.train.y\n",
    "valid_xs_final = load_pickle(path / \"valid_xs_final.pkl\")\n",
    "valid_y = to.valid.y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_vars = [\"SalesID\", \"MachineID\"]\n",
    "xs_final_time = xs_final.drop(time_vars, axis=1)\n",
    "valid_xs_time = valid_xs_final.drop(time_vars, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_forest(\n",
    "    xs: pd.DataFrame,\n",
    "    y: pd.Series,\n",
    "    n_estimators: int = 40,\n",
    "    max_samples: int = 200_000,\n",
    "    max_features: float = 0.5,\n",
    "    min_samples_leaf: int = 5,\n",
    "    **kwargs\n",
    "):\n",
    "    return RandomForestRegressor(\n",
    "        n_jobs=-1,\n",
    "        n_estimators=n_estimators,\n",
    "        max_samples=max_samples,\n",
    "        max_features=max_features,\n",
    "        min_samples_leaf=min_samples_leaf,\n",
    "        oob_score=True\n",
    "    ).fit(xs, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.229011"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = random_forest(xs_final_time, y)\n",
    "m_rmse(model, valid_xs_time, valid_y)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading data and setup - Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/david/anaconda3/envs/fast.ai/lib/python3.9/site-packages/pandas/core/arrays/categorical.py:2747: FutureWarning: The `inplace` parameter in pandas.Categorical.set_categories is deprecated and will be removed in a future version. Removing unused categories will always return a new Categorical object.\n",
      "  res = method(*args, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "df_nn = pd.read_csv(path / \"TrainAndValid.csv\", low_memory=False)\n",
    "\n",
    "df_nn[\"ProductSize\"] = df_nn[\"ProductSize\"].astype(\"category\")\n",
    "\n",
    "sizes = \"Large\", \"Large / Medium\", \"Medium\", \"Small\", \"Mini\", \"Compact\"\n",
    "df_nn[\"ProductSize\"].cat.set_categories(sizes, ordered=True, inplace=True)\n",
    "\n",
    "dep_var = \"SalePrice\"\n",
    "\n",
    "df_nn[dep_var] = np.log(df_nn[dep_var])\n",
    "df_nn = add_datepart(df_nn, \"saledate\")\n",
    "\n",
    "condition = (df_nn.saleYear < 2011) | (df_nn.saleMonth < 10)\n",
    "# np.where is a useful function that returns (as the first element of a tuple) the indices of all True values\n",
    "train_idx = np.where(condition)[0]\n",
    "valid_idx = np.where(~condition)[0]\n",
    "\n",
    "splits = (list(train_idx), list(valid_idx))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/david/anaconda3/envs/fast.ai/lib/python3.9/site-packages/fastai/tabular/core.py:279: FutureWarning: The default dtype for empty Series will be 'object' instead of 'float64' in a future version. Specify a dtype explicitly to silence this warning.\n",
      "  to.conts = (to.conts-self.means) / self.stds\n"
     ]
    }
   ],
   "source": [
    "df_nn_final = df_nn[list(xs_final_time.columns) + [dep_var]]\n",
    "cont_nn, cat_nn = cont_cat_split(df_nn_final, max_card=9000, dep_var=dep_var)\n",
    "\n",
    "batch_size = 1024\n",
    "\n",
    "procs_nn = [Categorify, FillMissing, Normalize]\n",
    "\n",
    "to_nn = TabularPandas(\n",
    "    df_nn_final,\n",
    "    procs_nn,\n",
    "    cat_nn,\n",
    "    cont_nn,\n",
    "    splits=splits,\n",
    "    y_names=dep_var\n",
    ")\n",
    "\n",
    "dls = to_nn.dataloaders(batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"nn\"\n",
    "\n",
    "learner = tabular_learner(\n",
    "    dls,\n",
    "    y_range=(8, 12),\n",
    "    layers=[500, 250],\n",
    "    n_out=1,\n",
    "    loss_func=F.mse_loss\n",
    ")\n",
    "\n",
    "learner.load(model_name)\n",
    "\n",
    "preds, targs = learner.get_preds()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combine models\n",
    "\n",
    "One minor issue we have to be aware of is that our PyTorch model and our sklearn model create data of different types: PyTorch gives us a rank-2 tensor (i.e, a column matrix), whereas NumPy gives us a rank-1 array (a vector). `squeeze` removes any unit axes from a tensor, and to_np converts it into a NumPy array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_preds = model.predict(valid_xs_time)\n",
    "ens_preds = (to_np(preds.squeeze()) + rf_preds) / 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.220122"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This gives us a better result than either model achieved on its own:\n",
    "\n",
    "r_mse(ens_preds, valid_y)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Boosting\n",
    "\n",
    "So far our approach to ensembling has been to use bagging, which involves combining many models (each trained on a different data subset) together by averaging them.\n",
    "\n",
    "There is another important approach to ensembling, called boosting, where we add models instead of averaging them. Here is how boosting works:\n",
    "\n",
    "- Train a small model that underfits your dataset.\n",
    "- Calculate the predictions in the training set for this model.\n",
    "- Subtract the predictions from the targets; these are called the \"residuals\" and represent the error for each point in the training set.\n",
    "- Go back to step 1, but instead of using the original targets, use the residuals as the targets for the training.\n",
    "- Continue doing this until you reach some stopping criterion, such as a maximum number of trees, or you observe your validation set error getting worse.\n",
    "\n",
    "\n",
    "Using this approach, each new tree will be attempting to fit the error of all of the previous trees combined.\n",
    "\n",
    "Note that, unlike with random forests, with this approach there is nothing to stop us from overfitting."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusion\n",
    "\n",
    "\n",
    "We suggest starting your analysis with a random forest. This will give you a strong baseline, and you can be confident that it's a reasonable starting point. You can then use that model for feature selection and partial dependence analysis, to get a better understanding of your data.\n",
    "\n",
    "From that foundation, you can try neural nets and GBMs, and if they give you significantly better results on your validation set in a reasonable amount of time, you can use them. If decision tree ensembles are working well for you, try adding the embeddings for the categorical variables to the data, and see if that helps your decision trees learn better."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fast.ai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
